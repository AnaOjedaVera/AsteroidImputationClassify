{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bc3d60-365e-4cd8-926c-582746e5f05d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0183f6-26f7-406a-996d-b1d6f128faa4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712c0ea-f725-469b-aee5-a9820c4b2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune the variance smoothing parameter of GaussianNB.\n",
    "#    Values range logarithmically from 1e-10 to 1e-6.\n",
    "param_grid = {'var_smoothing': np.logspace(-12, -6, num=2)}\n",
    "gnb = GaussianNB()\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "grid_search = GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final GaussianNB model with the best parameter.\n",
    "best_gnb = GaussianNB(**best_params)\n",
    "best_gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_gnb.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_gnb, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_gnb.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_gnb.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-GNB-A.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters (variance smoothing):\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    # Create a table that shows only the Mean and Std for each metric.\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time  \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5158b-ced6-4229-ae7d-ffe3a3c3969b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7458d-2ade-4fc1-b164-84e34916a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune the variance smoothing parameter of GaussianNB.\n",
    "#    Values range logarithmically from 1e-10 to 1e-6.\n",
    "param_grid = {'var_smoothing': np.logspace(-12, -6, num=2)}\n",
    "gnb = GaussianNB()\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "grid_search = GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final GaussianNB model with the best parameter.\n",
    "best_gnb = GaussianNB(**best_params)\n",
    "best_gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_gnb.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_gnb, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_gnb.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_gnb.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-GNB.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters (variance smoothing):\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    # Create a table that shows only the Mean and Std for each metric.\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time  \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607f3fb-26b9-472e-922a-fad900279eb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ed186-6448-4675-9e41-ad46eeeb2ca2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0ffde-c072-48fe-bb2a-46020f5466ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Multilayer Perceptron.\n",
    "# Hyperparameter grid: hidden_layer_sizes, learning_rate_init, solver, max_iter.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(32, 32), (64, 64), (32, 32, 32), (64, 64, 64)],\n",
    "    'learning_rate_init': [0.01, 0.05, 0.1],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n",
    "mlp = MLPClassifier(random_state=42, activation='relu')\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Multilayer Perceptron model with the best parameters.\n",
    "best_mlp = MLPClassifier(random_state=42, **best_params, activation='relu', tol=1e-3)\n",
    "best_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_mlp, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_mlp.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_mlp.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLP-A.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4003c-7f8e-452e-a7c0-87d7fb3d52b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845e135-951d-4409-9510-1ea5cd02c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Multilayer Perceptron.\n",
    "# Hyperparameter grid: hidden_layer_sizes, learning_rate_init, solver, max_iter.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(32, 32), (64, 64), (32, 32, 32), (64, 64, 64)],\n",
    "    'learning_rate_init': [0.01, 0.05, 0.1],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n",
    "mlp = MLPClassifier(random_state=42, activation='relu')\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Multilayer Perceptron model with the best parameters.\n",
    "best_mlp = MLPClassifier(random_state=42, **best_params, activation='relu', tol=1e-3)\n",
    "best_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_mlp, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_mlp.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_mlp.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLP.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5886c3-812e-4de0-9337-35212f09eb28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca87087-844e-4cee-8a32-76672d397f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8828c-8da8-48d3-bd61-def37c0cb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for multinomial logistic regression.\n",
    "C_values = list(range(5, 61, 5)) + list(np.logspace(-4, 4, 20))\n",
    "param_grid = {\n",
    "    'C': C_values,\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [10000, 25000, 50000]\n",
    "}\n",
    "\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final multinomial logistic regression model with the best parameters.\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "best_logreg = LogisticRegression(random_state=42, **best_params)\n",
    "best_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_logreg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_logreg, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_logreg.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_logreg.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLR-A.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73b72d-c310-4ee3-926d-93a88794f0ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a5860-88ab-44f3-b47e-d4355eece406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for multinomial logistic regression.\n",
    "C_values = list(range(5, 61, 5)) + list(np.logspace(-4, 4, 20))\n",
    "param_grid = {\n",
    "    'C': C_values,\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [10000, 25000, 50000]\n",
    "}\n",
    "\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final multinomial logistic regression model with the best parameters.\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "best_logreg = LogisticRegression(random_state=42, **best_params)\n",
    "best_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_logreg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_logreg, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_logreg.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_logreg.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLR.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59259d2-58b6-4227-8669-979e5031df77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cdbfa-33b8-4b16-804e-4510a0d582a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe632c3-3f15-4baf-ab87-b4b6acb47e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Random Forest.\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, 15],  # Tree depth\n",
    "    'min_samples_split': [15, 20, 30, 40],  # Minimum samples to split\n",
    "    'min_samples_leaf': [10, 15, 20, 30],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', 0.5, None]  # Feature selection method\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Random Forest model with the best parameters.\n",
    "best_rf = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_rf.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_rf.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-RF-A.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93664e65-8cd0-47ec-bcdb-12dec2db6d52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f156334-3d14-40c1-9464-fb7cfaa57fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Random Forest.\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, 15],  # Tree depth\n",
    "    'min_samples_split': [15, 20, 30, 40],  # Minimum samples to split\n",
    "    'min_samples_leaf': [10, 15, 20, 30],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', 0.5, None]  # Feature selection method\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Random Forest model with the best parameters.\n",
    "best_rf = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_rf.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_rf.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-RF.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85703650-519c-4ce8-9cdf-89c7b5c3f1fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dcd39c-579f-41b5-bdf9-aeb88e818efa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3094d-0bd2-4997-9bf7-cacc48245bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for SVM.\n",
    "# Hyperparameter grid: kernel (linear or rbf), regularization parameter 'C' (6 to 24),\n",
    "# and for the RBF kernel, gamma as 'scale' or 'auto'.\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': list(range(6, 25))},\n",
    "    {'kernel': ['rbf'], 'C': list(range(6, 25)), 'gamma': ['scale', 'auto']}\n",
    "]\n",
    "svc = SVC(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final SVM model with the best parameters.\n",
    "best_svm = SVC(random_state=42, **best_params)\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Balanced Accuracy: {:.4f}\".format(test_balanced_accuracy))\n",
    "print(\"F1 Score (weighted): {:.4f}\".format(test_f1))\n",
    "print(\"MCC: {:.4f}\".format(test_mcc))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_svm, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_svm.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_svm.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"02-SVM-A.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Balanced Accuracy: {test_balanced_accuracy:.4f}\n",
    "F1 Score (weighted): {test_f1:.4f}\n",
    "MCC: {test_mcc:.4f}\n",
    "\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35378b53-b130-4081-aa33-31064a9922e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726f5a7-a389-4a9f-b050-26e6726a8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-Base.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for SVM.\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': list(range(6, 25)) + list(np.logspace(-4, 4, 20))},\n",
    "    {'kernel': ['rbf'], 'C': list(range(6, 25)) + list(np.logspace(-4, 4, 20)), 'gamma': ['scale', 'auto']}\n",
    "]\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final SVM model with the best parameters.\n",
    "best_svm = SVC(random_state=42, **best_params)\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Balanced Accuracy: {:.4f}\".format(test_balanced_accuracy))\n",
    "print(\"F1 Score (weighted): {:.4f}\".format(test_f1))\n",
    "print(\"MCC: {:.4f}\".format(test_mcc))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_svm, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_svm.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_svm.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-SVM.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Balanced Accuracy: {test_balanced_accuracy:.4f}\n",
    "F1 Score (weighted): {test_f1:.4f}\n",
    "MCC: {test_mcc:.4f}\n",
    "\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
